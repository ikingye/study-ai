{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 梯度下降\n\n梯度下降是一种常用的优化算法，通过迭代更新模型参数，使得损失函数逐渐减小。梯度下降分为批量梯度下降、小批量梯度下降和随机梯度下降。"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 批量梯度下降\n\n批量梯度下降是最基本的梯度下降方法，每次迭代使用整个训练集计算梯度，更新模型参数。"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# 设置字体和解决负号显示问题\nplt.rcParams['font.sans-serif'] = 'Hiragino Sans GB'\nplt.rcParams['axes.unicode_minus'] = False\n\n# 定义损失函数\ndef loss_function(w):\n    return w**2 + 4*w + 4\n\n# 定义梯度\ndef gradient(w):\n    return 2*w + 4\n\n# 批量梯度下降\nw = 10  # 初始值\nlearning_rate = 0.1\niterations = 20\nw_values = [w]\nloss_values = [loss_function(w)]\nfor i in range(iterations):\n    w -= learning_rate * gradient(w)\n    w_values.append(w)\n    loss_values.append(loss_function(w))\n\n# 绘制损失函数和梯度下降过程\nplt.figure(figsize=(10, 5))\nplt.plot(w_values, loss_values, 'o-', label='批量梯度下降')\nplt.xlabel('参数 w')\nplt.ylabel('损失函数值')\nplt.title('批量梯度下降过程')\nplt.legend()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 小批量梯度下降\n\n小批量梯度下降是一种改进的梯度下降方法，每次迭代使用一部分训练集计算梯度，更新模型参数，具有更快的收敛速度和更好的处理大规模数据集的能力。"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from sklearn.linear_model import SGDRegressor\n\n# 生成示例数据\nnp.random.seed(0)\nX = 2 * np.random.rand(100, 1)\ny = 4 + 3 * X + np.random.randn(100, 1)\n\n# 拟合小批量梯度下降模型\nsgd_reg = SGDRegressor(max_iter=1000, tol=1e-3)\nsgd_reg.fit(X, y.ravel())\n\n# 输出模型参数\nprint(f'截距: {sgd_reg.intercept_}')\nprint(f'系数: {sgd_reg.coef_}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 随机梯度下降\n\n随机梯度下降是一种改进的梯度下降算法，每次迭代只使用一个样本更新模型参数，具有更快的收敛速度和更好的处理大规模数据集的能力。"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from sklearn.linear_model import SGDRegressor\n\n# 生成示例数据\nnp.random.seed(0)\nX = 2 * np.random.rand(100, 1)\ny = 4 + 3 * X + np.random.randn(100, 1)\n\n# 拟合随机梯度下降模型\nsgd_reg = SGDRegressor(max_iter=1000, tol=1e-3)\nsgd_reg.fit(X, y.ravel())\n\n# 输出模型参数\nprint(f'截距: {sgd_reg.intercept_}')\nprint(f'系数: {sgd_reg.coef_}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "通过以上示例，我们了解了批量梯度下降、小批量梯度下降和随机梯度下降的基本原理和应用。在实际应用中，选择合适的梯度下降算法可以显著提高模型的训练效率和性能。"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

