{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 优化算法\n\n优化算法在机器学习模型的训练过程中起着至关重要的作用。常见的优化算法包括梯度下降、随机梯度下降和贝叶斯优化等。在本节中，我们将详细介绍这些优化算法的基本原理和应用。"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 梯度下降\n\n梯度下降是一种常用的优化算法，通过迭代更新模型参数，使得损失函数逐渐减小。梯度下降分为批量梯度下降、小批量梯度下降和随机梯度下降。"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# 设置字体和解决负号显示问题\nplt.rcParams['font.sans-serif'] = 'Hiragino Sans GB'\nplt.rcParams['axes.unicode_minus'] = False\n\n# 定义损失函数\ndef loss_function(w):\n    return w**2 + 4*w + 4\n\n# 定义梯度\ndef gradient(w):\n    return 2*w + 4\n\n# 梯度下降\nw = 10  # 初始值\nlearning_rate = 0.1\niterations = 20\nw_values = [w]\nloss_values = [loss_function(w)]\nfor i in range(iterations):\n    w -= learning_rate * gradient(w)\n    w_values.append(w)\n    loss_values.append(loss_function(w))\n\n# 绘制损失函数和梯度下降过程\nplt.figure(figsize=(10, 5))\nplt.plot(w_values, loss_values, 'o-', label='梯度下降')\nplt.xlabel('参数 w')\nplt.ylabel('损失函数值')\nplt.title('梯度下降过程')\nplt.legend()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 随机梯度下降\n\n随机梯度下降是一种改进的梯度下降算法，每次迭代只使用一个样本更新模型参数，具有更快的收敛速度和更好的处理大规模数据集的能力。"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from sklearn.linear_model import SGDRegressor\n\n# 生成示例数据\nnp.random.seed(0)\nX = 2 * np.random.rand(100, 1)\ny = 4 + 3 * X + np.random.randn(100, 1)\n\n# 拟合随机梯度下降模型\nsgd_reg = SGDRegressor(max_iter=1000, tol=1e-3)\nsgd_reg.fit(X, y.ravel())\n\n# 输出模型参数\nprint(f'截距: {sgd_reg.intercept_}')\nprint(f'系数: {sgd_reg.coef_}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 贝叶斯优化\n\n贝叶斯优化是一种基于贝叶斯统计的优化方法，通过构建代理模型，迭代地选择最优的超参数组合。贝叶斯优化比网格搜索和随机搜索更高效，适用于高维度的超参数空间。"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from skopt import BayesSearchCV\n\n# 定义贝叶斯优化的搜索空间\nsearch_spaces = {\n    'n_estimators': (50, 200),\n    'max_depth': (10, 30),\n    'min_samples_split': (2, 10)\n}\n\n# 进行贝叶斯优化\nbayes_search = BayesSearchCV(estimator=rf, search_spaces=search_spaces, n_iter=10, cv=5, scoring='accuracy', random_state=42)\nbayes_search.fit(X, y)\n\n# 输出最优参数和最优准确率\nprint(f'最优参数: {bayes_search.best_params_}')\nprint(f'最优准确率: {bayes_search.best_score_:.2f}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "通过以上示例，我们了解了梯度下降、随机梯度下降和贝叶斯优化的基本原理和应用。在实际应用中，选择合适的优化算法可以显著提高模型的训练效率和性能。"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

