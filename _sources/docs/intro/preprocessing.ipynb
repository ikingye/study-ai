{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理\n",
    "\n",
    "数据预处理是机器学习过程中非常重要的一步。在这一部分，我们将介绍数据预处理的几个关键步骤，并通过实际代码示例来演示每个步骤的实现方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入必要的库\n",
    "\n",
    "在开始数据预处理之前，我们需要导入一些常用的库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理缺失值\n",
    "\n",
    "在数据集中，缺失值是一个常见的问题。我们可以使用多种方法来处理缺失值，包括删除包含缺失值的样本，或用其他值（如均值或中位数）填补缺失值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个示例数据集\n",
    "data = {'A': [1, 2, np.nan, 4], 'B': [5, np.nan, np.nan, 8], 'C': [10, 11, 12, 13]}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"原始数据:\")\n",
    "print(df)\n",
    "\n",
    "# 使用均值填补缺失值\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "print(\"\\n填补缺失值后的数据:\")\n",
    "print(df_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据规范化\n",
    "\n",
    "数据规范化是将不同量纲的数据转换到同一量纲上，以避免某些特征对模型的影响过大。常见的规范化方法有标准化（Standardization）和归一化（Normalization）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个示例数据集\n",
    "data = {'A': [1, 2, 3, 4], 'B': [100, 200, 300, 400]}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"原始数据:\")\n",
    "print(df)\n",
    "\n",
    "# 标准化数据\n",
    "scaler = StandardScaler()\n",
    "df_standardized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "print(\"\\n标准化后的数据:\")\n",
    "print(df_standardized)\n",
    "\n",
    "# 归一化数据\n",
    "scaler = MinMaxScaler()\n",
    "df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "print(\"\\n归一化后的数据:\")\n",
    "print(df_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征提取与选择\n",
    "\n",
    "特征提取是指从原始数据中提取有用的信息，转换为模型可以处理的特征。特征选择是从众多特征中选择对模型预测最有帮助的特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个示例数据集\n",
    "data = {'A': [1, 2, 3, 4], 'B': [100, 200, 300, 400], 'C': ['red', 'green', 'blue', 'yellow']}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"原始数据:\")\n",
    "print(df)\n",
    "\n",
    "# 使用 OneHotEncoder 对分类数据进行编码\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded_features = encoder.fit_transform(df[['C']])\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(['C']))\n",
    "\n",
    "# 将编码后的特征与原数据集连接\n",
    "df_combined = df.drop('C', axis=1).join(encoded_df)\n",
    "print(\"\\n编码后的数据:\")\n",
    "print(df_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上述步骤，我们可以看到数据预处理的基本流程和方法。在实际的机器学习项目中，数据预处理是至关重要的一步，它直接影响到模型的性能和效果。在接下来的章节中，我们将进一步探讨具体的机器学习算法及其应用。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

